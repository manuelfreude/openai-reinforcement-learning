{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG open ai environment solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources: \n",
    "\n",
    "### https://towardsdatascience.com/reinforcement-learning-w-keras-openai-actor-critic-models-f084612cfd69\n",
    "### https://github.com/pemami4911/deep-rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ornstein Uhlenbeck action noise \n",
    "\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer dense_1387: expected min_ndim=2, found ndim=1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-9dbfa9b11065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-97-9dbfa9b11065>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Taxi-v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mactor_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mnum_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-9dbfa9b11065>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, sess)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_state_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_actor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_actor_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_actor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-9dbfa9b11065>\u001b[0m in \u001b[0;36mcreate_actor_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_actor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mstate_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mh3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    488\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected min_ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0;31m# Check dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer dense_1387: expected min_ndim=2, found ndim=1"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "solving pendulum using actor-critic model\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers.merge import Add, Multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "import csv\n",
    "\n",
    "# determines how to assign values to each state, i.e. takes the state\n",
    "# and action (two-input model) and determines the corresponding value\n",
    "\n",
    "\n",
    "class ActorCritic:\n",
    "    def __init__(self, env, sess):\n",
    "        self.env  = env\n",
    "        self.sess = sess\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = .995\n",
    "        self.gamma = .95\n",
    "        self.tau   = .125\n",
    "                \n",
    "        # ===================================================================== #\n",
    "        #                               Actor Model                             #\n",
    "        # Chain rule: find the gradient of chaging the actor network params in  #\n",
    "        # getting closest to the final value network predictions, i.e. de/dA    #\n",
    "        # Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A act #\n",
    "        # ===================================================================== #\n",
    "\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.actor_state_input, self.actor_model = self.create_actor_model()\n",
    "        _, self.target_actor_model = self.create_actor_model()\n",
    "\n",
    "        self.actor_critic_grad = tf.placeholder(tf.float32, \n",
    "            [None, self.env.action_space.shape[0]]) # where we will feed de/dC (from critic)\n",
    "        \n",
    "        actor_model_weights = self.actor_model.trainable_weights\n",
    "        self.actor_grads = tf.gradients(self.actor_model.output, \n",
    "            actor_model_weights, -self.actor_critic_grad) # dC/dA (from actor)\n",
    "        grads = zip(self.actor_grads, actor_model_weights)\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\n",
    "\n",
    "        # ===================================================================== #\n",
    "        #                              Critic Model                             #\n",
    "        # ===================================================================== #\n",
    "\n",
    "        self.critic_state_input, self.critic_action_input, \\\n",
    "            self.critic_model = self.create_critic_model()\n",
    "        _, _, self.target_critic_model = self.create_critic_model()\n",
    "\n",
    "        self.critic_grads = tf.gradients(self.critic_model.output, \n",
    "            self.critic_action_input) # where we calcaulte de/dC for feeding above\n",
    "        \n",
    "        # Initialize for later gradient calculations\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # ========================================================================= #\n",
    "    #                              Model Definitions                            #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def create_actor_model(self):\n",
    "        state_input = Input(shape=self.env.observation_space.shape)\n",
    "        h1 = Dense(24, activation='relu')(state_input)\n",
    "        h2 = Dense(48, activation='relu')(h1)\n",
    "        h3 = Dense(24, activation='relu')(h2)\n",
    "        output = Dense(self.env.action_space.shape[0], activation='relu')(h3)\n",
    "        \n",
    "        model = Model(inputs=state_input, outputs=output)\n",
    "        adam  = Adam(lr=0.001)\n",
    "        model.compile(loss=\"mse\", optimizer=adam)\n",
    "        return state_input, model\n",
    "\n",
    "    def create_critic_model(self):\n",
    "        state_input = Input(shape=self.env.observation_space.shape)\n",
    "        state_h1 = Dense(24, activation='relu')(state_input)\n",
    "        state_h2 = Dense(48)(state_h1)\n",
    "        \n",
    "        action_input = Input(shape=self.env.action_space.shape)\n",
    "        action_h1    = Dense(48)(action_input)\n",
    "        \n",
    "        merged    = Add()([state_h2, action_h1])\n",
    "        merged_h1 = Dense(24, activation='relu')(merged)\n",
    "        output = Dense(1, activation='relu')(merged_h1)\n",
    "        model  = Model(inputs=[state_input,action_input], outputs=output)\n",
    "        \n",
    "        adam  = Adam(lr=0.001)\n",
    "        model.compile(loss=\"mse\", optimizer=adam)\n",
    "        return state_input, action_input, model\n",
    "\n",
    "    # ========================================================================= #\n",
    "    #                               Model Training                              #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def remember(self, cur_state, action, reward, new_state, done):\n",
    "        self.memory.append([cur_state, action, reward, new_state, done])\n",
    "\n",
    "    def _train_actor(self, samples):\n",
    "        for sample in samples:\n",
    "            cur_state, action, reward, new_state, _ = sample\n",
    "            predicted_action = self.actor_model.predict(cur_state)\n",
    "            grads = self.sess.run(self.critic_grads, feed_dict={\n",
    "                self.critic_state_input:  cur_state,\n",
    "                self.critic_action_input: predicted_action\n",
    "            })[0]\n",
    "\n",
    "            self.sess.run(self.optimize, feed_dict={\n",
    "                self.actor_state_input: cur_state,\n",
    "                self.actor_critic_grad: grads\n",
    "            })\n",
    "            \n",
    "    def _train_critic(self, samples):\n",
    "        for sample in samples:\n",
    "            cur_state, action, reward, new_state, done = sample\n",
    "            if not done:\n",
    "                target_action = self.target_actor_model.predict(new_state)\n",
    "                future_reward = self.target_critic_model.predict(\n",
    "                    [new_state, target_action])[0][0]\n",
    "                reward += self.gamma * future_reward\n",
    "            self.critic_model.fit([cur_state, action], reward, verbose=0)\n",
    "\n",
    "    def train(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        rewards = []\n",
    "        #episode_reward = tf.Variable(0.)\n",
    "        #summary_reward = [rewards]\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        self._train_critic(samples)\n",
    "        self._train_actor(samples)\n",
    "        sess = tf.Session()\n",
    "        \n",
    "        #summary_rewards = build_summaries()\n",
    "        #print(summary_rewards)\n",
    "        #writer = tf.summary.FileWriter('./results', sess.graph)\n",
    "        \n",
    "            \n",
    "    # ========================================================================= #\n",
    "    #                         Target Model Updating                             #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def _update_actor_target(self):\n",
    "        actor_model_weights  = self.actor_model.get_weights()\n",
    "        actor_target_weights = self.target_critic_model.get_weights()\n",
    "        \n",
    "        for i in range(len(actor_target_weights)):\n",
    "            actor_target_weights[i] = actor_model_weights[i]\n",
    "        self.target_critic_model.set_weights(actor_target_weights)\n",
    "\n",
    "    def _update_critic_target(self):\n",
    "        critic_model_weights  = self.critic_model.get_weights()\n",
    "        critic_target_weights = self.critic_target_model.get_weights()\n",
    "        \n",
    "        for i in range(len(critic_target_weights)):\n",
    "            critic_target_weights[i] = critic_model_weights[i]\n",
    "        self.critic_target_model.set_weights(critic_target_weights)\n",
    "\n",
    "    def update_target(self):\n",
    "        self._update_actor_target()\n",
    "        self._update_critic_target()\n",
    "\n",
    "    # ========================================================================= #\n",
    "    #                              Model Predictions                            #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def act(self, cur_state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return self.actor_model.predict(cur_state)\n",
    "\n",
    "\n",
    "def main():\n",
    "    sess = tf.Session()\n",
    "    K.set_session(sess)\n",
    "    env = gym.make(\"Pendulum-v0\")\n",
    "    actor_critic = ActorCritic(env, sess)\n",
    "\n",
    "    num_trials = 10000\n",
    "    trial_len = 500\n",
    "    \n",
    "    cur_state = env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    file_output = 'Open AI DDPG Reward Capture.csv'\n",
    "    labels = ['episode', 'reward', 'average_reward']\n",
    "    \n",
    "    # initialize average rewards\n",
    "    avg_rewards = []\n",
    "    # initialize monitor for most recent rewards\n",
    "    samp_rewards = []\n",
    "    \n",
    "    with open(file_output, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(labels)\n",
    "        for episode in range(1, num_trials+1):            \n",
    "            cur_state = cur_state.reshape((1, env.observation_space.shape[0]))\n",
    "            \n",
    "            while True:\n",
    "                env.render() #you need to run this in the Python script\n",
    "                action = actor_critic.act(cur_state)\n",
    "                action = action.reshape((1, env.action_space.shape[0]))\n",
    "\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                new_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
    "\n",
    "                actor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "                actor_critic.train()\n",
    "\n",
    "                cur_state = new_state\n",
    "\n",
    "                samp_reward = reward\n",
    "                samp_rewards.append(samp_reward)\n",
    "                avg_reward = np.mean(samp_rewards)\n",
    "                avg_rewards.append(avg_reward)\n",
    "\n",
    "                # capture progress\n",
    "                print(episode, samp_reward, avg_reward)\n",
    "                writer.writerow([episode, samp_reward, avg_reward])\n",
    "                \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
